{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой домашке мы будем реализовывать один из самых простых бинарных классификаторов — логистическую регрессию и её обучение с помощью обычного (полного) и стохастического градиентных спусков.\n",
    "\n",
    "Если кратко, то обучение логистической регрессии с $L_2$-регуляризацией можно записать следующим образом:\n",
    "\n",
    "$$\n",
    "Q(w, X) = \\frac{1}{n} \\sum_{i=1}^{n} \\log (1 + \\exp(- y_i [\\langle w, x_i \\rangle + b])) + \\frac{\\lambda_2}{2} \\lVert w \\rVert _2^2 \\to \\min_w\n",
    "$$ при этом $L_2$ регуляризатор вычисляется следующим образом $\\lVert w \\rVert _2^2 = \\sum\\limits_{k=1}^d w_k^2$. Регуляризация нужна для того, чтобы не происходило переобучения (такое возможно в случае линейных моделей, хотя и происходит редко). Так может получиться, что признаки в данных линейно-зависимы, это значит, что одно и то же значение суммы с весами $\\langle w, x_i \\rangle = \\sum\\limits_{k=1}^d x_i w_i$ сразу для всех объектов $x_i$ можно получить при разных наборах весов $w_i$. Если такое возможно, то можно подобрать сколь угодно большие по модулю веса, но значение суммы будет одно и то же. Большие веса приводят к неустойчивости в предсказании. Регуляризатор не даёт весам стать большими. Обратите внимание, что свободный член $b$ не входит в регуляризацию.\n",
    "\n",
    "$\\lambda_2 - $ константа, коэффициент регуляризации, который выбирается перед началом обучения.\n",
    "\n",
    "Считаем, что $y_i \\in \\{-1, +1\\}$. Искать $w$ будем с помощью градиентного спуска:\n",
    "\n",
    "$$\n",
    "w^{(k+1)} = w^{(k)} - \\alpha \\nabla_w Q(w, X)\n",
    "$$\n",
    "\n",
    "В случае полного градиентного спуска $\\nabla_w Q(w, X)$ считается напрямую (как есть, то есть, используя все объекты выборки). В случае стохастического градиентного спуска $\\nabla_w Q(w, X) \\approx \\nabla_w q_{i_k} (w)$, где $i_k$ — случайно выбранный номер слагаемого из функционала (регуляризатор можно внести в сумму, предварительно умножив и разделив на $n$). Длину шага $\\alpha > 0$ в рамках данного задания предлагается брать равной некоторой малой константе.\n",
    "\n",
    "Градиент по весам $w$ для объекта $x_i$ считается по следующей формуле:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, x_i) = - \\frac{y_i x_i}{1 + \\exp(y_i (\\langle w, x_i \\rangle) + b)} + \\lambda_2 w\n",
    "$$\n",
    "\n",
    "Напоминаю, что градиент $-$ это вектор, составленный из производных функции по аргументам вектора, то есть выше просто сложены в вектор $d$ штук производных функции по переменным $w_k$.\n",
    "Градиент по свободному члену $b$ предлагается найти самостоятельно.\n",
    "\n",
    "В качестве критерия останова необходимо использовать (одновременно):\n",
    "- проверку на разность евклидовых норм весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$) — параметр tolerance\n",
    "- достижение максимального числа итераций (например, 10000) — параметр max\\_iter.\n",
    "\n",
    "Инициализировать веса можно случайным образом или нулевым вектором.\n",
    "\n",
    "Вероятность принадлежности объекта $x$ классу $+1$ вычисляется следующим образом:\n",
    "\n",
    "$$\n",
    "P(y = +1 | x) = \\frac{1}{1 + \\exp(- (\\langle w, x \\rangle + b))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: import everything you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogReg(BaseEstimator):\n",
    "    def __init__(self, lambda_2=1.0, gd_type='stochastic',\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, alpha=1e-3):\n",
    "        \"\"\"\n",
    "        lambda_2: L2 regularization param\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.w = None\n",
    "        # YOUR CODE: initialize all the variables you'd like to access in further methods\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (n, d)\n",
    "        y: np.array of shape (n)\n",
    "        ---\n",
    "        output: self\n",
    "        \"\"\"\n",
    "        # YOUR CODE: write the learning procedure\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (n, d)\n",
    "        ---\n",
    "        output: np.array of shape (n, 2) where\n",
    "        first column has probabilities of -1\n",
    "        second column has probabilities of +1\n",
    "        \"\"\"\n",
    "        # YOUR CODE: predict probabilities for objects from X of belonging to positive and negative classes\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (n, d)\n",
    "        ---\n",
    "        output: np.array of shape (n) where\n",
    "        \"\"\"\n",
    "        # YOUR CODE: predict class labels for objects from X\n",
    "\n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (n, d) (n can be equal to 1 if stochastic)\n",
    "        y: np.array of shape (n)\n",
    "        ---\n",
    "        output: np.array of shape (d), float\n",
    "        \"\"\"\n",
    "        # YOUR CODE: calculate the gradient of loss function with respect to model parameters\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (n, d)\n",
    "        y: np.array of shape (n)\n",
    "        ---\n",
    "        output: float\n",
    "        \"\"\"\n",
    "        # YOUR CODE: compute the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите свой код на датесете Iris, взяв из него только нулевой и первый классы (у нас всё-таки бинарная классификация) и не забудьте преобразовать метки нулевого класса в -1, чтобы совпадало с формулами выше. Здесь можно найти код для отрисовки пространства предсказаний, переиспользуйте его и нарисуйте такую же картинку для конкретно нашей версии датасета Iris (с двумя классами) и с использованием реализованной выше логистической регрессии.\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: draw the predictions for Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
